# VariantCallingSimulations
Scripts and code used to perform variant calling simulations for non-model organisms.

Most scripts (not awk though) that I've written have a `-h` option, so take a look, and let me know if anything is unclear!

## Categories
1. [Genome simulation](README.md#simulation-related-scripts)
1. [Variant calling evaluation](README.md#evaluation-related-scripts)

### Compilation of C++ scripts:

C++ programs here will be compiled by a simple call to `make`, although they won't be installed to a location in your PATH.

## Evaluation pipeline:

### Tasks

This pipeline, much like the [Pseudoreference Pipeline](https://github.com/YourePrettyGood/PseudoreferencePipeline/), performs several different tasks via `localArrayCall.sh`.  These tasks are:
1. `CLASSIFY`, which classifies sites based on error category, and calculates error rates (FPR, FDR, FNR, % masked) based on this
1. `INDELDIST`, which calculates the distance from a positive call (FP or TP) to the nearest called indel, and outputs the distribution of these distances
1. `STATS`, which extracts VCF statistics for each site, subsets them by error category, and provides commands for generating empirical distributions for these categories

### `localArrayCall.sh`

This script can be used to parallelize certain tasks, e.g. the `CLASSIFY` task, which calculates FPR, FDR, FNR, and % of sites masked for a given set of masking thresholds (and the unmasked values of these statistics) based on the output files from the [PseudoreferencePipeline](https://github.com/YourePrettyGood/PseudoreferencePipeline), and the `STATS` task extracts VCF statistics for the sites in the callable/classifiable BED (or all sites), then partitions these statistics by error category as indicated by the BEDs produced by `CLASSIFY`.

It operates in much the same way as `localArrayCall_v2.sh` from the [PseudoreferencePipeline](https://github.com/YourePrettyGood/PseudoreferencePipeline) in that it is fed a Task ID (e.g. `$SLURM_ARRAY_TASK_ID` when used in a SLURM array), a Job Type (e.g. `CLASSIFY`), a metadata file with one line per task/sample, and an extra two options.

For the `CLASSIFY` task, the extra options to `localArrayCall.sh` are the ground truth log (e.g. as produced by `diploidizeSNPlog`) and an optional BED file detailing which regions of the genome are considered callable/classifiable.

For example, when assessing error rates for resequencing data of an inbred line, the ground truth log can be left as an empty file (since there are no true positives for an inbred line), and if there are regions of residual heterozygosity, the classifiable BED file should exclude these intervals.

The metadata file is a tab-separated file with one row per sample, consisting of four columns:

1. Sample prefix (e.g. `10x/10x_1.5pct` for the 10x depth data of the 1.5% divergence simulation)
1. Reference genome FASTA (must have a FASTA index, as generated by `samtools faidx`)
1. Variant caller used (at present, only `HC` and `MPILEUP` are supported)
1. A comma-separated list of special options (e.g. `no_markdup,no_IR` if both marking of duplicates and indel realignment were skipped)

For the `STATS` task, several extra (mandatory unless stated otherwise) arguments exist:
1. The BED file of intervals that are classifiable/callable, or the keyword `all`. We use this option in our manuscript to reduce the regions under consideration to those having a 1-to-1 LAST alignment between the two reference genomes. This task does not, however, generate the empirical CDFs for you, since it is naive about which columns in the output correspond to which VCF statistics. A message at the end suggests one way to generate an empirical CDF from the output using Unix commands.
1. (optional) Comma-separated list of special options used for the `PSEUDOFASTA` task of the PseudoreferencePipeline which indicate which VCF to use (e.g. `no_markdup`, `no_IR`)
1. (optional, has default) The format string to use for extracting VCF statistics: for `MPILEUP`, this string is passed to `bcftools query` via the `-F` flag; for `HC`, this string is actually the string of `-F` and `-GF` options you would provide to GATK VariantsToTable.

Example calls for the `CLASSIFY` task:

```parallel -j7 --eta '/home/pfreilly/Downloads/Bioinformatics/Simulation/VariantCallingSimulations/localArrayCall.sh {1} CLASSIFY 0.5pctdiv_sim_evalbed_metadata.tsv Dyak_revised_diploid_SNPs.log 2> logs/0.5pctdiv_sim_CLASSIFY_v2_line{1}.stderr > logs/0.5pctdiv_sim_CLASSIFY_v2_line{1}.stdout' ::: {8..14}```

```parallel -j14 --eta '/home/pfreilly/Downloads/Bioinformatics/Simulation/VariantCallingSimulations/localArrayCall.sh {1} CLASSIFY Dyak_SD_Tai18E2_CLASSIFY_metadata.tsv GroundTruth/glaucus/DyakTai18E2_groundtruth.log DyakTai18E2_{2}_{3}.bed 2> logs/Dyak_SD_Tai18E2_CLASSIFY_v1_{2}_{3}_line{1}.stderr > logs/Dyak_SD_Tai18E2_CLASSIFY_v1_{2}_{3}_line{1}.stdout' ::: {1..14} ::: aligned ::: noncoding```

Example call for the `STATS` task:

```parallel -j14 --eta '/home/pfreilly/Downloads/Bioinformatics/Simulation/VariantCallingSimulations/localArrayCall.sh {1} STATS 0.5pctdiv_sim_stats_metadata.tsv Dyak_revised_diploid_SNPs.log 2> logs/0.5pctdiv_sim_STATS_line{1}.stderr > logs/0.5pctdiv_sim_STATS_line{1}.stdout' ::: {1..14}```

Example call for the `INDELDIST` task:

```parallel -j14 --eta '/home/pfreilly/Downloads/Bioinformatics/Simulation/VariantCallingSimulations/localArrayCall.sh {1} INDELDIST 0.5pctdiv_sim_evalbed_metadata.tsv Dyak_diploid_SNPs.log 2> logs/0.5pctdiv_sim_INDELDIST_line{1}.stderr > logs/0.5pctdiv_sim_INDELDIST_line{1}.stdout' ::: {1..14}```

## Simulation-related scripts:

### `simulateDivergedHaplotype.pl`

Given a haploid reference genome in unwrapped FASTA format, it simulates SNPs and indels with uniform spatial distributions along scaffolds, and in quantities determined by the percentage passed as the only positional argument. The indel rate is fixed at 0.04*SNP rate, and the length distribution of indels follows a modified geometric distribution with adjustable parameter (passed in by the `-g` option). The geometric distribution is modified such that the 0 class has density 0 (since a 0-length indel means nothing). The underlying implementation is simply that any time a 0 is drawn, the geometric is resampled until a non-zero length is generated. Insertions and deletions should occur at equal frequency (and the choice between the two is determined by rounding a random number between 0 and 1). The user can choose to omit or include indels (include indels by passing the `-n` flag).

The script outputs the simulated FASTA (unwrapped) to a file specified by the -o option. It also outputs a "SNP log" and an "indel log".

The SNP log is a TSV file with 4 columns:

1. Scaffold
1. Position in input FASTA coordinate space on the scaffold
1. Original allele
1. New allele

The indel log is a TSV file with 5 columns:

1. Scaffold
1. Position in input FASTA coordinate space on the scaffold
1. "ins" for insertion, or "del" for deletion
1. Indel length
1. String of bases that were inserted or deleted

Note that for deletions, column 2 denotes the first base that was deleted, not the base before the deletion. On the other hand, for insertions, column 2 denotes the base prior to the inserted bases.

Example call simulating 0.5% divergence, geometric parameter = 0.35, and including indels:

`simulateDivergedHaplotype.pl -i my_reference_unwrapped.fasta -o my_diverged_genome.fasta -n -g 0.35 0.5`

The SNP log for this example would be called `my_diverged_genome_SNPs.log` and the indel log would be called `my_diverged_genome_indels.log`.

### `mergeSNPlogs`

Mutations that occurred along the reference-ancestor branch need to be combined with mutations that occurred along the ancestor-haploid branch. On top of that, when indels are simulated along the reference-ancestor branch, this changes the coordinate space of the ancestor's FASTA, so we cannot simply perform a set join of the two SNP logs, we need to adjust the positions of ancestor-haploid branch SNPs back into the coordinate space of the reference. In order to perform this position adjustment, we need to know the positions and sizes of indels along the reference-ancestor branch, which we pass in via the ref-anc branch indel log (the `-i` option). Of course, we then need the ref-anc branch and anc-haploid branch SNP logs, which are passed in via the `-b` and `-c` options, respectively. The merged and adjusted SNP log is output to `STDOUT`, which we redirect to a file in the example call.

Example call:

`mergeSNPlogs -i my_ref_anc_indels.log -b my_ref_anc_SNPs.log -c my_anc_hap1_SNPs.log > haploid1_merged_SNPs.log`

### `diploidizeSNPlog`

Here we create the SNP log appropriate for a diploid formed by the two simulated haploids, in the coordinate space of the reference. In order to guarantee the correct scaffold ordering of the output, we pass in a FASTA index (.fai file, generated by `samtools faidx [reference FASTA]`) using the `-i` option. Then we pass in the two merged SNP logs using the `-a` and `-b` options (the order doesn't matter). Iterating over the scaffolds in the order presented in the .fai file, this program identifies SNPs present only in haploid A, SNPs present only in haploid B, and SNPs present in both haploids, and creates diploid records out of them. In particular, SNPs present only in one of the two haploids will be heterozygous positions in the diploid, consisting of the reference allele and the new allele, so they are output as the degenerate IUPAC code for that type of heterozygote. SNPs present in both haploids are output as the degeneration of the two alleles, so if the two alleles are the same, the site is output as homozygous for that allele, and if the alleles are different, the site is output as heterozygous for the two new alleles.

Note that this implies there may be sites with true genotype `1/2`, i.e. a triallelic site.

The output is a SNP log formatted as before, but containing IUPAC-encoded degenerate bases in the 4th column wherever heterozygous sites occur.

Example call:

`diploidizeSNPlog -i my_reference_unwrapped.fasta.fai -a haploid1_merged_SNPs.log -b haploid2_merged_SNPs.log > diploid_SNPs.log`

## Scripts underlying the evaluation pipeline:

### `compareSNPlogs`

Example call:

`compareSNPlogs -i [.fai FASTA index of the reference FASTA] -e [expected diploid SNP log] -o [observed INSNP file] -n [output FN log] -p [output FP log] -t [output TP log] -r [output ER log] -m [min depth]`

Here, the .fai file is used for calculating the number of true negatives, since we need the scaffold length to compute this from TP, FP, and FN, which we know from the logs.  Of course, you pass in the expected diploid SNP log generated by diploidizeSNPlog as above using the `-e` option, and you pass in the observed INSNP generated by applying one of the above awk scripts to the VCF output by the pseudoreference pipeline using the `-o` option.  The optional `-n`, `-p`, `-r`, and `-t` parameters allow you to get an INSNP-formatted log of all of the false negatives, false positives, true positives, and ambiguous miscalls (ERs), respectively, for further analysis of trends in these four categories.  For example, a significant fraction of the false negatives may be due to low sequencing depth, or low depth used by the variant caller, so you can process this INSNP with awk to generate a pattern file for fgrep, and then determine raw depth from an fgrep of the output of samtools depth, or determine the variant caller-used depth by using fgrep on the all-sites VCF.

The final parameter, `-m` or `--min_depth`, specifies a minimum depth, provided as a fifth column of the expected diploid SNP log, for a site to be considered "callable".  This allows us to calculate the statistics on only putatively "callable" sites -- sites with sufficient read coverage to have alleles detected.

Note that the INSNP-like log TSVs output can easily be converted to BED format, and the intervals of true negatives inferred from the set complement of the union of FP, FN, TP, and ER intervals. For example, to convert such an INSNP-like TSV to a BED:

`awk 'BEGIN{FS="\t";OFS="\t";}{print $1, $2-1, $2;}' Dyak_2Mreads/Dyak_2Mreads_MD_IR_mpileup_FPs.tsv | sort -k1,1 -k2,2n -k3,3n | bedtools merge -i - > Dyak_2Mreads/Dyak_2Mreads_MD_IR_mpileup_FPs_merged.bed`

And to find the set of TN intervals:

`cat Dyak_2Mreads/Dyak_2Mreads_MD_IR_mpileup_{ER,FN,FP,TP}s_merged.bed | sort -k1,1 -k2,2n -k3,3n | bedtools merge -i - | bedtools complement -i - -g <(cut -f1,2 Dyak_NY73_Quiver_Scaffolded_w60.fasta.fai) > Dyak_2Mreads/Dyak_2Mreads_MD_IR_mpileup_TNs_merged.bed`

### `closestIndelDistance.pl`

This script takes an INSNP of variant calls (via the `-i` argument) and a VCF (via the `-v` argument), and determines the distance for each SNP in the INSNP to the closest indel found in the VCF.

This script is used by the `INDELDIST` task.

### `groundTruthFromMAF.pl`

This script takes a pairwise alignment of two genomes generated using LAST (and in MAF format), and outputs a ground truth log in each appropriate coordinate space for divergent sites.

The two positional arguments correspond to the species labels prefixed to scaffold names during the swapping step when finding 1-to-1 alignments.

Example commands to generate such a MAF file for D. yakuba NY73PB v1 versus D. yakuba Tai18E2:

```bash
lastdb -P16 -uNEAR -R01 DyakTai18E2_NEAR Dyak_Tai18E2_renamed_nolowercase_w60.fasta > lastdb_DyakTai18E2_NEAR.stdout
last-train -P16 --revsym --matsym --gapsym -E0.05 -C2 DyakTai18E2_NEAR Dyak_NY73PB_renamed_nolowercase_w60.fasta > Dyak_NY73PB_renamed_nolowercase_vs_DyakTai18E2_NEAR.mat
lastal -P16 -m50 -E0.05 -C2 -p Dyak_NY73PB_renamed_nolowercase_vs_DyakTai18E2_NEAR.mat DyakTai18E2_NEAR Dyak_NY73PB_renamed_nolowercase_w60.fasta > lastal_DyakNY73PB_DyakTai18E2_NEAR.stdout
last-split -m1 lastal_DyakNY73PB_DyakTai18E2_NEAR.stdout > lastsplit_DyakNY73PB_DyakTai18E2_NEAR.maf
maf-swap lastsplit_DyakNY73PB_DyakTai18E2_NEAR.maf | awk '/^s/{$2=(++s % 2 ? "DyakNY73PB." : "DyakTai18E2.")$2} 1' | last-split -m1 | maf-swap > Dyak_NY73PB_vs_Dyak_Tai18E2_1to1_NEAR.maf
```

### `HC_histograms.awk` and `MPILEUP_histograms.awk`

After performing the `STATS` task of the pipeline, you may want to summarize the VCF statistic histograms produced to make marginal distributions for plotting in R. This can be achieved using these two awk scripts in a pipe chain.

The calls we used to generate TSVs from the D. yakuba-like (1 percent divergence from ref) simulations for import into R were:

`parallel -j38 --eta 'HC_histograms.awk -v "stat={3}" Dyak_{1}Mreads/Dyak_{1}Mreads_realigned_HC_{2}_stats.tsv | sort -k1,1g | uniq -c | awk -v "depth={1}" -v "class={2}" -v "stat={3}" -v "caller=HC" -F"\t" "BEGIN{OFS=FS;}{print ref, depth*5, caller, class, stat, \$2, \$1;}"' ::: 2 4 6 8 10 15 20 ::: ER FN FP TN TP ::: BaseQRankSum ClippingRankSum INFODP FS MQ MQRankSum QD ReadPosRankSum SOR FORMATDP GQ RGQ SB > 0.5pctdiv_sim_realigned_HC_stat_histograms.tsv`

`parallel -j38 --eta 'MPILEUP_histograms.awk -v "stat={3}" Dyak_{1}Mreads/Dyak_{1}Mreads_realigned_MPILEUP_{2}_stats.tsv | sort -k1,1g | uniq -c | awk -v "depth={1}" -v "class={2}" -v "stat={3}" -v "caller=MPILEUP" -F"\t" "BEGIN{OFS=FS;}{print ref, depth*5, caller, class, stat, \$2, \$1;}"' ::: 2 4 6 8 10 15 20 ::: ER FN FP TN TP ::: QUAL DP RPB MQB BQB MQSB SGB MQ0F GQ HOB MQ DP4 > 0.5pctdiv_sim_realigned_MPILEUP_stat_histograms.tsv`

`printf "Depth\tCaller\tCategory\tStatistic\tCount\tValue\n" > 0.5pctdiv_sim_realigned_ALL_stat_histograms.tsv`

`awk 'BEGIN{FS="\t";OFS=FS;}$5!="DP4"' 0.5pctdiv_sim_realigned_HC_stat_histograms.tsv 0.5pctdiv_sim_realigned_MPILEUP_stat_histograms.tsv | cut -f1 --complement >> 0.5pctdiv_sim_realigned_ALL_stat_histograms.tsv`

This last command simply concatenates the two TSVs, and eliminates the DP4 statistic, because the format of DP4 causes problems with R parsing.

### `subsetVCFstats.pl`

Example call:

`subsetVCFstats.pl -i Dyak_2Mreads/Dyak_2Mreads_realigned_HC_GGVCFs.vcf -b Dyak_2Mreads/Dyak_2Mreads_MD_IR_GATK_GGVCFs_FPs_merged.bed | less`

This script is fairly generic in that it will take any tab-separated file whose first two columns are Scaffold and Position, and will print out the lines of that file corresponding to the intervals provided in the BED file. The script was originally written to subset lines out of stats files made by `bcftools query` or GATK VariantsToTable, but can be applied to the VCF itself, or really any file fitting these criteria.

### `VCFtoUnfilteredINSNP_skipInsertions.awk`

Generates an unfiltered INSNP from the ERCGVCF.vcf produced directly by GATK HaplotypeCaller.

Example call:

`VCFtoUnfilteredINSNP_skipInsertions.awk Dyak_2Mreads/Dyak_2Mreads_realigned_HC_ERCGVCF.vcf > Dyak_2Mreads/Dyak_2Mreads_MD_IR_ERCGVCF_unfiltered_INSNP.tsv`

### `VCFtoUnfilteredINSNP_skipInsertions_GGVCFs.awk`

Generates an unfiltered INSNP from the GGVCFs.vcf produced by GATK GenotypeGVCFs.

Example call:

`VCFtoUnfilteredINSNP_skipInsertions_GGVCFs.awk Dyak_2Mreads/Dyak_2Mreads_realigned_HC_GGVCFs.vcf > Dyak_2Mreads/Dyak_2Mreads_MD_IR_GGVCFs_unfiltered_INSNP.tsv`

### `VCFtoUnfilteredINSNP_skipInsertions_samtools.awk`

Generates an unfiltered INSNP from the bgzipped VCF produced by BCFtools call.

Example call:

`zcat Dyak_2Mreads/Dyak_2Mreads_realigned_mpileupcall.vcf.gz | VCFtoUnfilteredINSNP_skipInsertions_samtools.awk > Dyak_2Mreads/Dyak_2Mreads_MD_IR_mpileup_unfiltered_INSNP.tsv`

